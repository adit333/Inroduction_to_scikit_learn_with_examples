{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid search is used when we want to find the best (combination of) hyperparameters. It is as simple as building many models with all combinations of hyperparameters and then chosing the setting which gives the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case of a kernel SVM with an RBF (radial basis function) kernel, as\n",
    "implemented in the SVC class. There two importrant hyperparameters: C for regularization and gamma for contolling the width. We wish to find the best combination for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a simple grid search just as for loops over the two parameters,\n",
    "training and evaluating a classifier for each combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 112 size of test set: 38\n",
      "Best score: 0.97\n",
      "Best parameters: {'C': 100, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Naive grid search implementation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "print(\"Size of training set: {} size of test set: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # For each combination of parameters, train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the SVC on the test set\n",
    "        score = svm.score(X_test, y_test)\n",
    "        \n",
    "        # If we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "            \n",
    "print(\"Best score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Danger of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a terrible mistake by using the test set for both finding the best hyperparameters and testing the model evaluation; so we are not using any unseen data for model perfomance evaluation.\n",
    "We can rectufy this by creating a third set called the validation set, and then finding the hyperparameters on the validation set, and then using the test set only for model evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 84   size of validation set: 28   size of test set: 38\n",
      "\n",
      "Best score on validation set: 0.96\n",
      "Best parameters:  {'C': 10, 'gamma': 0.001}\n",
      "Test set score with best parameters: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Split data into train+validation set and test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# Split train+validation set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( X_trainval, y_trainval, random_state=1)\n",
    "\n",
    "print(\"Size of training set: {}   size of validation set: {}   size of test set:\"\n",
    "        \" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # For each combination of parameters, train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # Evaluate the SVC on the validation set\n",
    "        score = svm.score(X_valid, y_valid)\n",
    "        # If we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "# Rebuild a model on the combined training and validation set and evaluate it on the test set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "\n",
    "test_score = svm.score(X_test, y_test)\n",
    "print(\"Best score on validation set: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "print(\"Test set score with best parameters: {:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score on the validation set is 96%: slightly lower than before, probably\n",
    "because we used less data to train the model (X_train is smaller now because we split\n",
    "our dataset twice). However, the score on the test set—the score that actually tells us\n",
    "how well we generalize—is even lower, at 92%. So we can only claim to classify new\n",
    "data 92% correctly, not 97% correctly as we thought before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better estimate of the generalization performance, instead of\n",
    "using a single split into a training and a validation set, we can use cross-validation to\n",
    "evaluate the performance of each parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # For each combination of parameters,train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n",
    "        \n",
    "        # Compute mean cross-validation accuracy\n",
    "        score = np.mean(scores)\n",
    "        \n",
    "        # If we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "            \n",
    "# Rebuild a model on the combined training and validation set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the accuracy of the SVM using a particular setting of C and gamma using\n",
    "five-fold cross-validation, we need to train 36 * 5 = 180 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each parameter setting, five accuracy values are computed,\n",
    "one for each split in the cross-validation. Then the mean validation accuracy is\n",
    "computed for each hyperparameter setting. The hyperparameters with the highest mean validation\n",
    "accuracy are chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because grid search with cross-validation is such a commonly used method to adjust\n",
    "hyperparameters, scikit-learn provides the GridSearchCV class, which implements it in\n",
    "the form of an estimator. To use the GridSearchCV class, you first need to specify the\n",
    "hyperparameters you want to search over using a dictionary. GridSearchCV will then perform\n",
    "all the necessary model fits. The keys of the dictionary are the names of hyperparameters\n",
    "we want to adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV will use cross-validation in place of the split into a training and validation\n",
    "set that we used before. However, we still need to split the data into a training\n",
    "and a test set, to avoid overfitting the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid_search object that we created behaves just like a classifier; we can call the\n",
    "standard methods fit, predict, and score on it. A scikit-learn estimator that is created using another estimator is called a meta-estimator e.g. GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'gamma': [0.001, 0.01, 0.1, 1, 10, 100]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the parameters using cross-validation, we actually found a model that achieves\n",
    "97% accuracy on the test set. The important thing here is that we did not use the\n",
    "test set to choose the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters that were found are scored in the best_params_ attribute, and the best cross-validation accuracy (the mean accuracy over the different splits for this hyperparameter setting) is stored in best_score_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the result of cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often helpful to visualize the results of cross-validation, to understand how the\n",
    "model generalization depends on the hyperparameters we are searching. As grid searches\n",
    "are quite expensive to run, often it is a good idea to start with a small grid. We can then inspect the results of the cross-validated grid search, and possibly expand our search. The results of a grid search can be found\n",
    "in the cv_results_ attribute, which is a dictionary storing all aspects of the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.001}</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.366403</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.01}</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.366403</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.1}</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.366403</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1}</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.366403</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 0.001, 'gamma': 10}</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.366403</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.001198      0.001165         0.000662        0.000550   0.001   \n",
       "1       0.001829      0.002273         0.000823        0.001645   0.001   \n",
       "2       0.001845      0.001512         0.000208        0.000415   0.001   \n",
       "3       0.001683      0.001408         0.000406        0.000498   0.001   \n",
       "4       0.001301      0.000375         0.001005        0.000013   0.001   \n",
       "\n",
       "  param_gamma                        params  split0_test_score  \\\n",
       "0       0.001  {'C': 0.001, 'gamma': 0.001}           0.347826   \n",
       "1        0.01   {'C': 0.001, 'gamma': 0.01}           0.347826   \n",
       "2         0.1    {'C': 0.001, 'gamma': 0.1}           0.347826   \n",
       "3           1      {'C': 0.001, 'gamma': 1}           0.347826   \n",
       "4          10     {'C': 0.001, 'gamma': 10}           0.347826   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.347826           0.363636           0.363636           0.409091   \n",
       "1           0.347826           0.363636           0.363636           0.409091   \n",
       "2           0.347826           0.363636           0.363636           0.409091   \n",
       "3           0.347826           0.363636           0.363636           0.409091   \n",
       "4           0.347826           0.363636           0.363636           0.409091   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.366403        0.022485               22  \n",
       "1         0.366403        0.022485               22  \n",
       "2         0.366403        0.022485               22  \n",
       "3         0.366403        0.022485               22  \n",
       "4         0.366403        0.022485               22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert to DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "# Show the first 5 rows\n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search over spaces that are not grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some case, searching over all possible combinations of hyperparameters simply deoes not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, SVC has a kernel hyperparameter, and\n",
    "depending on which kernel is chosen, other hyperparameters will be relevant. If linear kernel is used, then only the C hyperparameter is used. If RBF kernel is used, then both the C and gamma hyperparameters are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with these kinds of “conditional” parameters,\n",
    "GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\n",
    "list is expanded into an independent grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "{'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using GridSearchCV, we have a single split of the data into training and test sets,\n",
    "which might make our results unstable and make us depend too much on this single split of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go a step further, and instead of splitting the original data\n",
    "into training and test sets once, use multiple splits of cross-validation. This will result\n",
    "in what is called nested cross-validation. In nested cross-validation, there is an outer\n",
    "loop over splits of the data into training and test sets. For each of them, a grid search\n",
    "is run (which might result in different best parameters for each split in the outer\n",
    "loop). Then, for each outer split, the test set score using the best settings is reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this procedure is a list of scores—not a model.\n",
    "The scores tell us how well a model generalizes, given the best parameters found by the grid.\n",
    "We can not use it for predictions; however, it can be useful for evaluating how well a given model works on a particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing nested cross-validation in scikit-learn is straightforward. We call\n",
    "cross_val_score with an instance of GridSearchCV as the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.96666667 1.         0.9        0.96666667 1.        ]\n",
      "Mean cross-validation score:  0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing cross-validation and grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While running a grid search over many parameters and on large datasets can be computationally\n",
    "challenging, it is also embarrassingly parallel. This means that building a model using a particular parameter setting on a particular cross-validation split can\n",
    "be done completely independently from the other hyperparameter settings and models.\n",
    "You can make use of multiple cores in Grid\n",
    "SearchCV and cross_val_score by setting the n_jobs parameter to the number of\n",
    "CPU cores you want to use. You can set n_jobs=-1 to use all available cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that scikit-learn does not allow nesting of parallel operations.\n",
    "So, if you are using the n_jobs option on your model (for example, a random forest),\n",
    "you cannot use it in GridSearchCV to search over this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
