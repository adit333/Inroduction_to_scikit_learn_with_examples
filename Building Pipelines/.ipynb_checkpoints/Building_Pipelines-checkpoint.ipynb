{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline class is a class that\n",
    "allows “gluing” together multiple processing steps into a single scikit-learn estimator. The Pipeline class itself has fit, predict, and score methods and behaves just\n",
    "like any other model in scikit-learn. The most common use case of the Pipeline\n",
    "class is in chaining preprocessing steps (like scaling of the data) together with a\n",
    "supervised model like a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at how we can use the Pipeline class to express the workflow for training\n",
    "an SVM after scaling the data with MinMaxScaler (for now without the grid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and split the data\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build a pipeline object by providing it with a list of steps. Each step is a tuple\n",
    "containing a name (any string of your choosing1) and an instance of an estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\n",
    "and the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\n",
    "any other scikit-learn estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', MinMaxScaler()), ('svm', SVC())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe.fit first calls fit on the first step (the scaler), then transforms the training\n",
    "data using the scaler, and finally fits the SVM with the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score method on the pipeline first transforms the test data using the\n",
    "scaler, and then calls the score method on the SVM using the scaled test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pipeline, we reduced the\n",
    "code needed for our preprocessing + classification process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pipelines in Grid Searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pipeline in a grid search works the same way as using any other estimator. We\n",
    "define a hyperparameter grid to search over, and construct a GridSearchCV from the pipeline\n",
    "and the hyperparameter grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying the hyperparameter grid, we need to specify for each hyperparameter which step of the pipeline it\n",
    "belongs to. Both hyperparameters that we want to adjust, C and gamma, are hyperparameters of\n",
    "SVC, the second step; so, we gave this step the name \"svm\". The syntax to define a hyperparameter\n",
    "grid for a pipeline is to specify for each hyperparameter the step name, followed by __\n",
    "(a double underscore), followed by the hyperparameter name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy: 0.98\n",
      "Test set score: 0.97\n",
      "Best parameters: {'svm__C': 1, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each split in the cross-validation,\n",
    "the MinMaxScaler is refit with only the training splits and no information is leaked\n",
    "from the test split into the parameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Pipeline Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only requirement for estimators in a pipeline is that all but the last step need to\n",
    "have a transform method, so they can produce a new representation of the data that\n",
    "can be used in the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal code looks similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    X_transformed = X\n",
    "    for name, estimator in self.steps[:-1]:\n",
    "        # Iterate over all but the final step\n",
    "        # Fit and transform the data\n",
    "        X_transformed = estimator.fit_transform(X_transformed, y)\n",
    "        \n",
    "    # Fit the last step\n",
    "    self.steps[-1][1].fit(X_transformed, y)\n",
    "    \n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    X_transformed = X\n",
    "    for step in self.steps[:-1]:\n",
    "        # Iterate over all but the final step\n",
    "        # Transform the data\n",
    "        X_transformed = step[1].transform(X_transformed)\n",
    "        \n",
    "    # Fit the last step\n",
    "    return self.steps[-1][1].predict(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_pipeline, that will create a pipeline for us and automatically name\n",
    "each step based on its class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# standard syntax\n",
    "pipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "\n",
    "# Abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_short has steps that were automatically named. We can see the names of the\n",
    "steps by looking at the steps attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps:\n",
      "[('minmaxscaler', MinMaxScaler()), ('svc', SVC(C=100))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Step Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will want to inspect attributes of one of the steps of the pipeline—say, the\n",
    "coefficients of a linear model or the components extracted by PCA. The easiest way to\n",
    "access the steps in a pipeline is via the named_steps attribute, which is a dictionary\n",
    "from the step names to the estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps:\n",
      "[('standardscaler-1', StandardScaler()), ('pca', PCA(n_components=2)), ('standardscaler-2', StandardScaler())]\n",
      "components.shape: (2, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe.steps))\n",
    "\n",
    "# Fit the pipeline defined before to the cancer dataset\n",
    "pipe.fit(cancer.data)\n",
    "\n",
    "# Extract the first two principal components from the \"pca\" step\n",
    "components = pipe.named_steps[\"pca\"].components_\n",
    "\n",
    "print(\"components.shape: {}\".format(components.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Attributes in a Grid-Searched Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s grid search a LogisticRegression classifier on the cancer dataset,\n",
    "using Pipeline and StandardScaler to scale the data before passing it to the LogisticRegression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we used the\n",
    "make_pipeline function, the name of the LogisticRegression step in the pipeline is\n",
    "the lowercased class name, logisticregression. To tune the hyperparameter C, we therefore\n",
    "have to specify a hyperparameter grid for logisticregression__C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             param_grid={'logisticregression__C': [0.01, 0.1, 1, 10, 100]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=4)\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model found by\n",
    "GridSearchCV, trained on all the training data, is stored in grid.best_estimator_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression', LogisticRegression(C=1, max_iter=1000))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This best_estimator_ in our case is a pipeline with two steps, standardscaler and\n",
    "logisticregression. To access the logisticregression step, we can use the\n",
    "named_steps attribute of the pipeline, as explained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression step:\n",
      "LogisticRegression(C=1, max_iter=1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression step:\\n{}\".format(grid.best_estimator_.named_steps[\"logisticregression\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression coefficients:\n",
      "[[-0.43570655 -0.34266946 -0.40809443 -0.5344574  -0.14971847  0.61034122\n",
      "  -0.72634347 -0.78538827  0.03886087  0.27497198 -1.29780109  0.04926005\n",
      "  -0.67336941 -0.93447426 -0.13939555  0.45032641 -0.13009864 -0.10144273\n",
      "   0.43432027  0.71596578 -1.09068862 -1.09463976 -0.85183755 -1.06406198\n",
      "  -0.74316099  0.07252425 -0.82323903 -0.65321239 -0.64379499 -0.42026013]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression coefficients:\\n{}\".format(grid.best_estimator_.named_steps[\"logisticregression\"].coef_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
